{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 01: Data Ingestion & Database Loading\n",
        "\n",
        "## Purpose\n",
        "This notebook loads, cleans, and ingests CSV data into PostgreSQL with **correct column mappings from the start**.\n",
        "\n",
        "## Key Features\n",
        "- Loads CSV files (with original column names like `PL_YTD`, `MV_Base`)\n",
        "- Maps columns to database format during ingestion (`plytd`, `mvbase`)\n",
        "- Cleans data (duplicates, missing values, date parsing)\n",
        "- Loads into PostgreSQL with correct column names\n",
        "- Creates indexes for performance\n",
        "\n",
        "## Column Mapping Strategy\n",
        "**CRITICAL**: During ingestion, CSV columns are mapped to database column names:\n",
        "- `PL_YTD` → `plytd` (no underscore, lowercase)\n",
        "- `PL_MTD` → `plmtd`\n",
        "- `PL_QTD` → `plqtd`\n",
        "- `PL_DTD` → `pldtd`\n",
        "- `MV_Base` → `mvbase`\n",
        "- `MV_Local` → `mvlocal`\n",
        "- `PortfolioName` → `portfolioname`\n",
        "- All other columns → lowercase\n",
        "\n",
        "This ensures the database tables have the correct column names from the start, eliminating the need for normalization later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Imports loaded\n",
            "   Data directory: /Users/suren/Desktop/untitled folder 2/loop-task/data\n",
            "   Database: localhost:5432/funddb\n"
          ]
        }
      ],
      "source": [
        "# Cell 1: Imports and Setup\n",
        "import pandas as pd\n",
        "import json\n",
        "import re\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from datetime import datetime, timezone\n",
        "from sqlalchemy import create_engine, text, inspect, pool\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Project root\n",
        "project_root = Path.cwd().parent\n",
        "data_dir = project_root / \"data\"\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv(project_root.parent / \".env\")\n",
        "\n",
        "# Database URL\n",
        "DATABASE_URL = os.getenv(\"DATABASE_URL\", \"postgresql://postgres:postgres@localhost:5432/fund_data\")\n",
        "if DATABASE_URL.startswith(\"postgresql://\") and \"+psycopg\" not in DATABASE_URL:\n",
        "    DATABASE_URL = DATABASE_URL.replace(\"postgresql://\", \"postgresql+psycopg://\", 1)\n",
        "\n",
        "print(\"✅ Imports loaded\")\n",
        "print(f\"   Data directory: {data_dir}\")\n",
        "print(f\"   Database: {DATABASE_URL.split('@')[-1] if '@' in DATABASE_URL else DATABASE_URL}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Column Mapping Examples:\n",
            "  PL_YTD → plytd\n",
            "  PL_MTD → plmtd\n",
            "  MV_Base → mvbase\n",
            "  PortfolioName → portfolioname\n",
            "  Quantity → quantity\n"
          ]
        }
      ],
      "source": [
        "# Cell 2: Column Mapping Function\n",
        "# CRITICAL: Maps CSV column names to database column names\n",
        "\n",
        "def map_column_to_database(csv_col: str) -> str:\n",
        "    \"\"\"Map CSV column name to database column name.\n",
        "    \n",
        "    Database uses lowercase, no underscores for P&L columns.\n",
        "    \"\"\"\n",
        "    col_lower = csv_col.lower()\n",
        "    \n",
        "    # P&L columns: remove underscores\n",
        "    if \"pl_ytd\" in col_lower or col_lower == \"plytd\":\n",
        "        return \"plytd\"\n",
        "    elif \"pl_mtd\" in col_lower or col_lower == \"plmtd\":\n",
        "        return \"plmtd\"\n",
        "    elif \"pl_qtd\" in col_lower or col_lower == \"plqtd\":\n",
        "        return \"plqtd\"\n",
        "    elif \"pl_dtd\" in col_lower or col_lower == \"pldtd\":\n",
        "        return \"pldtd\"\n",
        "    # Market value columns\n",
        "    elif \"mv_base\" in col_lower or col_lower == \"mvbase\":\n",
        "        return \"mvbase\"\n",
        "    elif \"mv_local\" in col_lower or col_lower == \"mvlocal\":\n",
        "        return \"mvlocal\"\n",
        "    # All other columns: convert to lowercase\n",
        "    else:\n",
        "        return col_lower\n",
        "\n",
        "# Test mapping\n",
        "test_cols = [\"PL_YTD\", \"PL_MTD\", \"MV_Base\", \"PortfolioName\", \"Quantity\"]\n",
        "print(\"Column Mapping Examples:\")\n",
        "for col in test_cols:\n",
        "    print(f\"  {col} → {map_column_to_database(col)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Holdings loaded: 1022 rows, 25 columns\n",
            "   Original columns: ['AsOfDate', 'OpenDate', 'CloseDate', 'ShortName', 'PortfolioName']...\n",
            "✅ Columns mapped to database format\n",
            "   Database columns: ['asofdate', 'opendate', 'closedate', 'shortname', 'portfolioname']...\n",
            "   Key mappings: PL_YTD → plytd, MV_Base → mvbase\n",
            "⚠️  Removed 2 duplicate rows\n",
            "✅ Holdings ready: 1020 rows\n"
          ]
        }
      ],
      "source": [
        "# Cell 3: Load and Map Holdings Data\n",
        "\n",
        "holdings_file = data_dir / \"holdings.csv\"\n",
        "holdings_df = pd.read_csv(holdings_file)\n",
        "\n",
        "print(f\"✅ Holdings loaded: {len(holdings_df)} rows, {len(holdings_df.columns)} columns\")\n",
        "print(f\"   Original columns: {list(holdings_df.columns[:5])}...\")\n",
        "\n",
        "# Map columns to database names\n",
        "column_mapping = {col: map_column_to_database(col) for col in holdings_df.columns}\n",
        "holdings_df = holdings_df.rename(columns=column_mapping)\n",
        "\n",
        "print(f\"✅ Columns mapped to database format\")\n",
        "print(f\"   Database columns: {list(holdings_df.columns[:5])}...\")\n",
        "print(f\"   Key mappings: PL_YTD → plytd, MV_Base → mvbase\")\n",
        "\n",
        "# Remove duplicates\n",
        "original_count = len(holdings_df)\n",
        "holdings_df = holdings_df.drop_duplicates()\n",
        "removed = original_count - len(holdings_df)\n",
        "if removed > 0:\n",
        "    print(f\"⚠️  Removed {removed} duplicate rows\")\n",
        "\n",
        "# Add ingestion timestamp\n",
        "holdings_df['ingested_at'] = datetime.now(timezone.utc)\n",
        "\n",
        "print(f\"✅ Holdings ready: {len(holdings_df)} rows\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Trades loaded: 649 rows, 31 columns\n",
            "   Original columns: ['id', 'RevisionId', 'AllocationId', 'TradeTypeName', 'SecurityId']...\n",
            "✅ Columns mapped to database format\n",
            "   Database columns: ['id', 'revisionid', 'allocationid', 'tradetypename', 'securityid']...\n",
            "✅ Trades ready: 649 rows\n"
          ]
        }
      ],
      "source": [
        "# Cell 4: Load and Map Trades Data\n",
        "\n",
        "trades_file = data_dir / \"trades.csv\"\n",
        "trades_df = pd.read_csv(trades_file)\n",
        "\n",
        "print(f\"✅ Trades loaded: {len(trades_df)} rows, {len(trades_df.columns)} columns\")\n",
        "print(f\"   Original columns: {list(trades_df.columns[:5])}...\")\n",
        "\n",
        "# Map columns to database names\n",
        "column_mapping = {col: map_column_to_database(col) for col in trades_df.columns}\n",
        "trades_df = trades_df.rename(columns=column_mapping)\n",
        "\n",
        "print(f\"✅ Columns mapped to database format\")\n",
        "print(f\"   Database columns: {list(trades_df.columns[:5])}...\")\n",
        "\n",
        "# Remove duplicates\n",
        "original_count = len(trades_df)\n",
        "trades_df = trades_df.drop_duplicates()\n",
        "removed = original_count - len(trades_df)\n",
        "if removed > 0:\n",
        "    print(f\"⚠️  Removed {removed} duplicate rows\")\n",
        "\n",
        "# Add ingestion timestamp\n",
        "trades_df['ingested_at'] = datetime.now(timezone.utc)\n",
        "\n",
        "print(f\"✅ Trades ready: {len(trades_df)} rows\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Data cleaned\n",
            "   Holdings: 1020 rows\n",
            "   Trades: 649 rows\n"
          ]
        }
      ],
      "source": [
        "# Cell 5: Clean Data (Date Parsing, Type Conversion)\n",
        "\n",
        "def clean_dataframe(df: pd.DataFrame, name: str) -> pd.DataFrame:\n",
        "    \"\"\"Clean dataframe: parse dates, convert types.\"\"\"\n",
        "    df = df.copy()\n",
        "    \n",
        "    # Parse date columns\n",
        "    date_cols = [col for col in df.columns if 'date' in col.lower()]\n",
        "    for col in date_cols:\n",
        "        if col in df.columns:\n",
        "            try:\n",
        "                # Try common date formats\n",
        "                df[col] = pd.to_datetime(df[col], format='%m/%d/%y', errors='coerce')\n",
        "                if df[col].isna().all():\n",
        "                    df[col] = pd.to_datetime(df[col], format='%m/%d/%Y', errors='coerce')\n",
        "            except:\n",
        "                df[col] = pd.to_datetime(df[col], errors='coerce')\n",
        "    \n",
        "    # Clean string columns\n",
        "    string_cols = df.select_dtypes(include=['object']).columns\n",
        "    for col in string_cols:\n",
        "        if col != 'ingested_at':\n",
        "            df[col] = df[col].astype(str).str.strip()\n",
        "            df[col] = df[col].replace(['', 'nan', 'None', 'null', 'NULL'], np.nan)\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Clean both dataframes\n",
        "holdings_df = clean_dataframe(holdings_df, \"Holdings\")\n",
        "trades_df = clean_dataframe(trades_df, \"Trades\")\n",
        "\n",
        "print(\"✅ Data cleaned\")\n",
        "print(f\"   Holdings: {len(holdings_df)} rows\")\n",
        "print(f\"   Trades: {len(trades_df)} rows\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Database connected\n",
            "✅ Holdings loaded to database: 1020 rows\n",
            "✅ Trades loaded to database: 649 rows\n"
          ]
        }
      ],
      "source": [
        "# Cell 6: Connect to Database and Load Data\n",
        "\n",
        "# Create database engine\n",
        "engine = create_engine(\n",
        "    DATABASE_URL,\n",
        "    poolclass=pool.QueuePool,\n",
        "    pool_size=10,\n",
        "    max_overflow=20,\n",
        "    pool_pre_ping=True,\n",
        "    echo=False\n",
        ")\n",
        "\n",
        "# Test connection\n",
        "with engine.connect() as conn:\n",
        "    conn.execute(text(\"SELECT 1\"))\n",
        "print(\"✅ Database connected\")\n",
        "\n",
        "# Load holdings\n",
        "holdings_df.to_sql(\n",
        "    \"fund_holdings\",\n",
        "    engine,\n",
        "    if_exists=\"replace\",\n",
        "    index=False,\n",
        "    chunksize=1000\n",
        ")\n",
        "print(f\"✅ Holdings loaded to database: {len(holdings_df)} rows\")\n",
        "\n",
        "# Load trades\n",
        "trades_df.to_sql(\n",
        "    \"fund_trades\",\n",
        "    engine,\n",
        "    if_exists=\"replace\",\n",
        "    index=False,\n",
        "    chunksize=1000\n",
        ")\n",
        "print(f\"✅ Trades loaded to database: {len(trades_df)} rows\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Holdings indexes created\n",
            "✅ Trades indexes created\n",
            "\n",
            "✅ Database tables verified\n",
            "   Holdings columns: 26\n",
            "   Key columns: ['mvlocal', 'mvbase', 'pldtd', 'plqtd', 'plmtd']\n",
            "   Trades columns: 32\n",
            "\n",
            "✅ Row counts verified\n",
            "   Holdings: 1020 rows\n",
            "   Trades: 649 rows\n"
          ]
        }
      ],
      "source": [
        "# Cell 7: Create Indexes and Verify\n",
        "\n",
        "# Create indexes for performance\n",
        "with engine.connect() as conn:\n",
        "    # Holdings indexes\n",
        "    conn.execute(text(\"CREATE INDEX IF NOT EXISTS idx_holdings_portfolio ON fund_holdings(portfolioname)\"))\n",
        "    conn.execute(text(\"CREATE INDEX IF NOT EXISTS idx_holdings_date ON fund_holdings(asofdate)\"))\n",
        "    conn.commit()\n",
        "    print(\"✅ Holdings indexes created\")\n",
        "    \n",
        "    # Trades indexes\n",
        "    conn.execute(text(\"CREATE INDEX IF NOT EXISTS idx_trades_portfolio ON fund_trades(portfolioname)\"))\n",
        "    conn.execute(text(\"CREATE INDEX IF NOT EXISTS idx_trades_date ON fund_trades(tradedate)\"))\n",
        "    conn.commit()\n",
        "    print(\"✅ Trades indexes created\")\n",
        "\n",
        "# Verify table structure\n",
        "inspector = inspect(engine)\n",
        "holdings_cols = [col['name'] for col in inspector.get_columns('fund_holdings')]\n",
        "trades_cols = [col['name'] for col in inspector.get_columns('fund_trades')]\n",
        "\n",
        "print(f\"\\n✅ Database tables verified\")\n",
        "print(f\"   Holdings columns: {len(holdings_cols)}\")\n",
        "print(f\"   Key columns: {[c for c in holdings_cols if 'pl' in c.lower() or 'mv' in c.lower()][:5]}\")\n",
        "print(f\"   Trades columns: {len(trades_cols)}\")\n",
        "\n",
        "# Verify row counts\n",
        "with engine.connect() as conn:\n",
        "    holdings_count = conn.execute(text(\"SELECT COUNT(*) FROM fund_holdings\")).scalar()\n",
        "    trades_count = conn.execute(text(\"SELECT COUNT(*) FROM fund_trades\")).scalar()\n",
        "    \n",
        "print(f\"\\n✅ Row counts verified\")\n",
        "print(f\"   Holdings: {holdings_count} rows\")\n",
        "print(f\"   Trades: {trades_count} rows\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
